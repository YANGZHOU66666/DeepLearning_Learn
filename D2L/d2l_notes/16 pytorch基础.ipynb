{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 pytorch基础\n",
    "\n",
    "## 1. 层和块\n",
    "\n",
    "单独的层：接受输入 -> 产生输出\n",
    "\n",
    "块：由单个或多个层组成，可以看作是层的组合，可以递归组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0937,  0.1797, -0.0505,  0.1489, -0.0124,  0.0220,  0.0561,  0.1562,\n",
       "         -0.1167, -0.0302],\n",
       "        [-0.0859,  0.1789, -0.0683,  0.0039,  0.0464, -0.1014,  0.0598,  0.2357,\n",
       "          0.0031, -0.0190]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义块\n",
    "\n",
    "块是nn.Module的子类，必须具有：\n",
    "\n",
    "1. 一个__init__方法，用于创建模型参数，其中必须先调用super().__init__()\n",
    "\n",
    "2. 一个forward方法，用于定义前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2158,  0.2293,  0.1619, -0.0421, -0.3486,  0.2489, -0.5542,  0.1339,\n",
      "         -0.0258, -0.0579],\n",
      "        [-0.0101,  0.3037,  0.1762,  0.1428, -0.3536,  0.0250, -0.2700, -0.2530,\n",
      "         -0.0080,  0.1015]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MLP_1(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(in_features, hidden_features)\n",
    "        self.linear_2 = nn.Linear(hidden_features, out_features)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear_2(F.relu(self.linear_1(X))) # F.relu()和nn.ReLU()的区别：前者是函数可以直接调用，后者是类，必须要实例化的对象\n",
    "    \n",
    "net = MLP_1(20, 256, 10)\n",
    "X = torch.randn(size=(2, 20))\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 顺序块(Sequential)\n",
    "\n",
    "Sequential模块可以将多个网络层组合成一个网络，将这些层存入_modules字典中\n",
    "\n",
    "在forward方法中，按照顺序执行它们\n",
    "\n",
    "以下手搓一个MySequential块:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2604],\n",
       "        [0.2672]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            self._modules[idx] = module\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "    \n",
    "net = MySequential(nn.Linear(10,512),nn.ReLU(),nn.Linear(512,1))\n",
    "net(torch.rand(2,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在前向传播函数中执行代码\n",
    "\n",
    "一些更底层、更自定义化的实现方式，而非拘束于调用封装的层、函数等\n",
    "\n",
    "如下例：\n",
    "\n",
    "实现 $ w_1(ReLU(w_{rand}(w_1x+b_1)))+b_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0602, -0.2257, -0.1723,  0.1205,  0.0465, -0.2156, -0.0561, -0.0472,\n",
       "          0.0984,  0.0034, -0.2141,  0.0454,  0.2163, -0.0227,  0.1513,  0.1046,\n",
       "          0.0141,  0.1319,  0.1106, -0.1454],\n",
       "        [ 0.0317, -0.2820, -0.4336,  0.6500,  0.0677,  0.2772, -0.4030, -0.7115,\n",
       "         -0.2275,  0.0409, -0.0601, -0.2752,  0.2198, -0.1042,  0.3759, -0.0593,\n",
       "         -0.0254,  0.0237,  0.3004,  0.2488]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(20,20) # 这里两个线性层参数共享\n",
    "        self.rand_w = torch.rand(20,20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        h = F.relu(torch.matmul(X,self.rand_w))\n",
    "        return self.linear(h)\n",
    "    \n",
    "net = FixedHiddenMLP()\n",
    "net(torch.rand(2,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 参数管理\n",
    "\n",
    "### 参数访问\n",
    "\n",
    "net[i]可以访问第i层的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0163,  0.0499, -0.0591,  0.1766, -0.0457,  0.0576, -0.0720, -0.0080,\n",
       "                       -0.1819, -0.2467, -0.1974, -0.1756, -0.1169, -0.1266, -0.0771, -0.1057]])),\n",
       "             ('bias', tensor([0.0731]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4,16),nn.ReLU(),nn.Linear(16,1))\n",
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "访问参数的w、b值及它们的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1488, -0.4096, -0.1873,  0.1729],\n",
      "        [-0.3364,  0.0039, -0.3798,  0.1748],\n",
      "        [-0.2610,  0.2074, -0.1086,  0.3270],\n",
      "        [-0.3054, -0.4790,  0.2081, -0.4523],\n",
      "        [-0.4647, -0.2731, -0.3505, -0.4309],\n",
      "        [-0.2665,  0.4562,  0.4150,  0.3067],\n",
      "        [-0.1316, -0.1673, -0.0766, -0.2751],\n",
      "        [-0.1977, -0.3928, -0.3569,  0.2064],\n",
      "        [ 0.2682, -0.1237, -0.1002, -0.3673],\n",
      "        [-0.2009, -0.4034,  0.4111,  0.3932],\n",
      "        [ 0.2700, -0.0105, -0.0394,  0.1753],\n",
      "        [ 0.0795, -0.0129,  0.0066,  0.4801],\n",
      "        [ 0.4254, -0.3943, -0.4794, -0.2830],\n",
      "        [ 0.3051, -0.3782, -0.4022, -0.4164],\n",
      "        [ 0.0654,  0.2872,  0.1431,  0.3079],\n",
      "        [ 0.3066,  0.3419,  0.0046, -0.2031]], requires_grad=True)\n",
      "tensor([-0.1969,  0.4057, -0.4210,  0.1963, -0.4943,  0.3071,  0.2810, -0.0128,\n",
      "         0.0702,  0.4643,  0.4901,  0.4625,  0.2548,  0.2357,  0.3704,  0.2693])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(net[0].weight)\n",
    "print(net[0].bias.data)\n",
    "print(net[0].bias.grad==None) # 还没有进行反向传播，故暂时没有梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', torch.Size([16, 4])), ('bias', torch.Size([16]))]\n",
      "[('0.weight', torch.Size([16, 4])), ('0.bias', torch.Size([16])), ('2.weight', torch.Size([1, 16])), ('2.bias', torch.Size([1]))]\n"
     ]
    }
   ],
   "source": [
    "print([(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print([(name,param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这也告诉我们可以如下操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1488, -0.4096, -0.1873,  0.1729],\n",
      "        [-0.3364,  0.0039, -0.3798,  0.1748],\n",
      "        [-0.2610,  0.2074, -0.1086,  0.3270],\n",
      "        [-0.3054, -0.4790,  0.2081, -0.4523],\n",
      "        [-0.4647, -0.2731, -0.3505, -0.4309],\n",
      "        [-0.2665,  0.4562,  0.4150,  0.3067],\n",
      "        [-0.1316, -0.1673, -0.0766, -0.2751],\n",
      "        [-0.1977, -0.3928, -0.3569,  0.2064],\n",
      "        [ 0.2682, -0.1237, -0.1002, -0.3673],\n",
      "        [-0.2009, -0.4034,  0.4111,  0.3932],\n",
      "        [ 0.2700, -0.0105, -0.0394,  0.1753],\n",
      "        [ 0.0795, -0.0129,  0.0066,  0.4801],\n",
      "        [ 0.4254, -0.3943, -0.4794, -0.2830],\n",
      "        [ 0.3051, -0.3782, -0.4022, -0.4164],\n",
      "        [ 0.0654,  0.2872,  0.1431,  0.3079],\n",
      "        [ 0.3066,  0.3419,  0.0046, -0.2031]])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict()['0.weight'].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从嵌套块收集参数\n",
    "\n",
    "使用`net.add_module()`方法\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1684],\n",
       "        [0.1684]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2,4)\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.参数初始化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
